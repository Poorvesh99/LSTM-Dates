{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ccc8deb-304c-43fd-b799-7d8e15e6a9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from utils import *\n",
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c1bc18b-cfb2-4de9-8c4f-1e361e7a69b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "learning_rate = 0.1\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 200\n",
    "dataset_size = 50000\n",
    "# max length on input sequence\n",
    "Tx = 30\n",
    "# length of output sequence\n",
    "Ty = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24f418fb-f3c5-453e-9ac8-0de947aa1dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 50000/50000 [00:01<00:00, 43673.05it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "769f6392-c1d7-4a1b-9131-8a11d56aa8e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('may 24 1892', '1892-05-24'),\n",
       " ('june-22-1859', '1859-06-22'),\n",
       " ('april 12 1974', '1974-04-12'),\n",
       " ('19 march 1983', '1983-03-19'),\n",
       " ('wednesday august 5 1953', '1953-08-05'),\n",
       " ('sunday june 26 1881', '1881-06-26'),\n",
       " ('tuesday october 24 1843', '1843-10-24'),\n",
       " ('30 december 1827', '1827-12-30'),\n",
       " ('sunday july 25 2010', '2010-07-25'),\n",
       " ('tuesday may 19 1992', '1992-05-19')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84100510-6d9c-48c1-a4ec-60d3366dd2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_org, Y_org, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
    "# Xoh and Yoh is one hot representation X and Y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "772bb985-7472-4c70-a4a6-30895fb04ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1655/1021108513.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y = torch.tensor(Y_org, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "Xoh = torch.tensor(Xoh, dtype=torch.float32)\n",
    "Y = torch.tensor(Y_org, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f4ee229-394f-4827-95cd-56b460a708b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "        self.Y = torch.tensor(Y, dtype=torch.long).to(device)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b70fb53-7c63-49eb-b231-e53ae46404b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1655/2152136669.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.X = torch.tensor(X, dtype=torch.float32).to(device)\n",
      "/tmp/ipykernel_1655/2152136669.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.Y = torch.tensor(Y, dtype=torch.long).to(device)\n"
     ]
    }
   ],
   "source": [
    "train_data = dataset(Xoh, Y)\n",
    "\n",
    "data_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "121583b3-8e41-4b9b-85a4-41500f0ac636",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_a = 32         # hidden units for pre_attention_lstm/encoder\n",
    "n_s = 64         # hidden units for post_attention_lstm/decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e846884f-51c9-476c-9e29-6fe25c1f3af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat vector for attention mechanism\n",
    "class RepeatVector(nn.Module):\n",
    "    def __init__(self, n):\n",
    "        super(RepeatVector, self).__init__()\n",
    "        self.n = n\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, features)\n",
    "        return x.unsqueeze(1).repeat(1, self.n, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b6a1735-8fdd-48a2-aaa6-5aa9b1a41883",
   "metadata": {},
   "outputs": [],
   "source": [
    "class modelf(nn.Module):\n",
    "    def __init__(self, Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
    "        super(modelf, self).__init__()\n",
    "\n",
    "        self.Tx = Tx\n",
    "        self.Ty = Ty\n",
    "        self.n_a = n_a\n",
    "        self.n_s = n_s\n",
    "        self.human_vocab_size = human_vocab_size\n",
    "        self.machine_vocab_size = machine_vocab_size\n",
    "        \n",
    "        # one_step_attention layers:\n",
    "        self.repeator = RepeatVector(self.Tx)\n",
    "        repeator = RepeatVector(self.Tx)\n",
    "        self.linear1 = nn.Linear((2*self.n_a) + self.n_s, 10)\n",
    "        self.linear2 = nn.Linear(10, 1)\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        # forward layers\n",
    "        self.pre_attention_lstm = nn.LSTM(input_size=self.human_vocab_size, hidden_size=self.n_a, batch_first=True, bidirectional=True)\n",
    "        self.post_attention_lstm = nn.LSTM(2* self.n_a , self.n_s, batch_first=True)\n",
    "        self.output_layer = nn.Linear(self.n_s, self.machine_vocab_size)\n",
    "        self.softmax_main = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def one_step_attention(self, a, s_prev):\n",
    "        # this attention mechnism for lstm\n",
    "        \n",
    "        # add's dimenison 1 making (1,2) to (1, 3, 2)\n",
    "        s_prev = self.repeator(s_prev)\n",
    "        concat = torch.cat([a, s_prev],dim=-1)\n",
    "       \n",
    "        e = self.linear1(concat)\n",
    "        e = self.tanh(e)\n",
    "        \n",
    "        energies = self.linear2(e)\n",
    "        energies = self.relu(energies)\n",
    "        # softmax on dimension 1\n",
    "        alphas = self.softmax(energies)\n",
    "        # this is dot product\n",
    "        context = torch.sum(alphas*a, dim=1, keepdim=True)\n",
    "        \n",
    "        return context\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        a, _ = self.pre_attention_lstm(X)\n",
    "\n",
    "        batch_size = X.shape[0]\n",
    "        s = torch.zeros(batch_size, n_s).to(device)\n",
    "        c = torch.zeros(batch_size, n_s).to(device)\n",
    "\n",
    "        for t in range(self.Ty):\n",
    "            # one setp attention\n",
    "            context = self.one_step_attention(a, s)\n",
    "        \n",
    "            _, (s, c) =  self.post_attention_lstm(context, (s.unsqueeze(0), c.unsqueeze(0)))\n",
    "            s, c = s.squeeze(0), c.squeeze(0)\n",
    "            \n",
    "            out = self.output_layer(s)\n",
    "            \n",
    "            outputs.append(out)\n",
    "\n",
    "        \n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        return outputs\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9165302-b11b-4fee-9419-8d29d2704599",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_vocab_size = len(human_vocab)\n",
    "machine_vocab_size = len(machine_vocab)\n",
    "model = modelf(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "564e33fb-e869-4bf8-a3ee-5dbf0bf695a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "modelf                                   [31064, 10, 11]           --\n",
       "├─LSTM: 1-1                              [31064, 30, 64]           18,432\n",
       "├─RepeatVector: 1-2                      [31064, 30, 64]           --\n",
       "├─Linear: 1-3                            [31064, 30, 10]           1,290\n",
       "├─Tanh: 1-4                              [31064, 30, 10]           --\n",
       "├─Linear: 1-5                            [31064, 30, 1]            11\n",
       "├─ReLU: 1-6                              [31064, 30, 1]            --\n",
       "├─Softmax: 1-7                           [31064, 30, 1]            --\n",
       "├─LSTM: 1-8                              [31064, 1, 64]            33,280\n",
       "├─Linear: 1-9                            [31064, 11]               715\n",
       "├─RepeatVector: 1-10                     [31064, 30, 64]           --\n",
       "├─Linear: 1-11                           [31064, 30, 10]           (recursive)\n",
       "├─Tanh: 1-12                             [31064, 30, 10]           --\n",
       "├─Linear: 1-13                           [31064, 30, 1]            (recursive)\n",
       "├─ReLU: 1-14                             [31064, 30, 1]            --\n",
       "├─Softmax: 1-15                          [31064, 30, 1]            --\n",
       "├─LSTM: 1-16                             [31064, 1, 64]            (recursive)\n",
       "├─Linear: 1-17                           [31064, 11]               (recursive)\n",
       "├─RepeatVector: 1-18                     [31064, 30, 64]           --\n",
       "├─Linear: 1-19                           [31064, 30, 10]           (recursive)\n",
       "├─Tanh: 1-20                             [31064, 30, 10]           --\n",
       "├─Linear: 1-21                           [31064, 30, 1]            (recursive)\n",
       "├─ReLU: 1-22                             [31064, 30, 1]            --\n",
       "├─Softmax: 1-23                          [31064, 30, 1]            --\n",
       "├─LSTM: 1-24                             [31064, 1, 64]            (recursive)\n",
       "├─Linear: 1-25                           [31064, 11]               (recursive)\n",
       "├─RepeatVector: 1-26                     [31064, 30, 64]           --\n",
       "├─Linear: 1-27                           [31064, 30, 10]           (recursive)\n",
       "├─Tanh: 1-28                             [31064, 30, 10]           --\n",
       "├─Linear: 1-29                           [31064, 30, 1]            (recursive)\n",
       "├─ReLU: 1-30                             [31064, 30, 1]            --\n",
       "├─Softmax: 1-31                          [31064, 30, 1]            --\n",
       "├─LSTM: 1-32                             [31064, 1, 64]            (recursive)\n",
       "├─Linear: 1-33                           [31064, 11]               (recursive)\n",
       "├─RepeatVector: 1-34                     [31064, 30, 64]           --\n",
       "├─Linear: 1-35                           [31064, 30, 10]           (recursive)\n",
       "├─Tanh: 1-36                             [31064, 30, 10]           --\n",
       "├─Linear: 1-37                           [31064, 30, 1]            (recursive)\n",
       "├─ReLU: 1-38                             [31064, 30, 1]            --\n",
       "├─Softmax: 1-39                          [31064, 30, 1]            --\n",
       "├─LSTM: 1-40                             [31064, 1, 64]            (recursive)\n",
       "├─Linear: 1-41                           [31064, 11]               (recursive)\n",
       "├─RepeatVector: 1-42                     [31064, 30, 64]           --\n",
       "├─Linear: 1-43                           [31064, 30, 10]           (recursive)\n",
       "├─Tanh: 1-44                             [31064, 30, 10]           --\n",
       "├─Linear: 1-45                           [31064, 30, 1]            (recursive)\n",
       "├─ReLU: 1-46                             [31064, 30, 1]            --\n",
       "├─Softmax: 1-47                          [31064, 30, 1]            --\n",
       "├─LSTM: 1-48                             [31064, 1, 64]            (recursive)\n",
       "├─Linear: 1-49                           [31064, 11]               (recursive)\n",
       "├─RepeatVector: 1-50                     [31064, 30, 64]           --\n",
       "├─Linear: 1-51                           [31064, 30, 10]           (recursive)\n",
       "├─Tanh: 1-52                             [31064, 30, 10]           --\n",
       "├─Linear: 1-53                           [31064, 30, 1]            (recursive)\n",
       "├─ReLU: 1-54                             [31064, 30, 1]            --\n",
       "├─Softmax: 1-55                          [31064, 30, 1]            --\n",
       "├─LSTM: 1-56                             [31064, 1, 64]            (recursive)\n",
       "├─Linear: 1-57                           [31064, 11]               (recursive)\n",
       "├─RepeatVector: 1-58                     [31064, 30, 64]           --\n",
       "├─Linear: 1-59                           [31064, 30, 10]           (recursive)\n",
       "├─Tanh: 1-60                             [31064, 30, 10]           --\n",
       "├─Linear: 1-61                           [31064, 30, 1]            (recursive)\n",
       "├─ReLU: 1-62                             [31064, 30, 1]            --\n",
       "├─Softmax: 1-63                          [31064, 30, 1]            --\n",
       "├─LSTM: 1-64                             [31064, 1, 64]            (recursive)\n",
       "├─Linear: 1-65                           [31064, 11]               (recursive)\n",
       "├─RepeatVector: 1-66                     [31064, 30, 64]           --\n",
       "├─Linear: 1-67                           [31064, 30, 10]           (recursive)\n",
       "├─Tanh: 1-68                             [31064, 30, 10]           --\n",
       "├─Linear: 1-69                           [31064, 30, 1]            (recursive)\n",
       "├─ReLU: 1-70                             [31064, 30, 1]            --\n",
       "├─Softmax: 1-71                          [31064, 30, 1]            --\n",
       "├─LSTM: 1-72                             [31064, 1, 64]            (recursive)\n",
       "├─Linear: 1-73                           [31064, 11]               (recursive)\n",
       "├─RepeatVector: 1-74                     [31064, 30, 64]           --\n",
       "├─Linear: 1-75                           [31064, 30, 10]           (recursive)\n",
       "├─Tanh: 1-76                             [31064, 30, 10]           --\n",
       "├─Linear: 1-77                           [31064, 30, 1]            (recursive)\n",
       "├─ReLU: 1-78                             [31064, 30, 1]            --\n",
       "├─Softmax: 1-79                          [31064, 30, 1]            --\n",
       "├─LSTM: 1-80                             [31064, 1, 64]            (recursive)\n",
       "├─Linear: 1-81                           [31064, 11]               (recursive)\n",
       "==========================================================================================\n",
       "Total params: 53,728\n",
       "Trainable params: 53,728\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 28.14\n",
       "==========================================================================================\n",
       "Input size (MB): 141.65\n",
       "Forward/backward pass size (MB): 1483.62\n",
       "Params size (MB): 0.21\n",
       "Estimated Total Size (MB): 1625.48\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model , input_size = Xoh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b38a3de5-fe5a-450f-bfa0-54bed5aabcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e864bfb-8a07-4f99-b27d-73b623c3c87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, loss:0.5941\n",
      "epoch:50, loss:0.2805\n",
      "epoch:100, loss:0.367\n",
      "epoch:150, loss:0.4914\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(inputs)\n",
    "\n",
    "        output_flat = out.reshape(-1, out.shape[-1])\n",
    "        labels_flat = labels.reshape(-1)\n",
    "\n",
    "        loss = criterion(output_flat, labels_flat)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    if epoch % 50 == 0 or epoch == EPOCHS-1:\n",
    "        print(f\"epoch:{epoch}, loss:{round(total_loss/1000, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb7e6d0-2d3b-4a22-bb15-f5655da93891",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"models/model_weights_1100.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a826a4ea-ab61-47e7-a315-d6c9dfed71a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, sentence):\n",
    "    model.eval()\n",
    "    x_enc = string_to_int(sentence, Tx, human_vocab)\n",
    "    x_enc = np.array(list(map(lambda x: np.eye(len(human_vocab))[x], x_enc)))\n",
    "    X_tensor = torch.tensor(x_enc, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_tensor)\n",
    "        if isinstance(outputs, list):\n",
    "            outputs = torch.stack(outputs, dim=1)\n",
    "        preds = torch.argmax(outputs, dim=-1).squeeze(0).cpu().numpy()\n",
    "    return ''.join(int_to_string(preds, inv_machine_vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aab44b-197c-4c31-8b96-ef89ca0de188",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = translate(model, \"19 3 2003\")\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d1678e-e01a-42a1-940f-f6e72f398efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = ['3 May 1979', '5 Apr 09', '20th Feb 2016', 'Wed 10 Jul 2007', '30 SEPT 2027', '1 jnu 2030']\n",
    "for example in examples:\n",
    "    print(f\"Input: {example} -> Output: {translate(model, example)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea0a916-8420-4f02-832a-248a7dd18a81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
