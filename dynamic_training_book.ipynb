{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ccc8deb-304c-43fd-b799-7d8e15e6a9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from d_utils import *\n",
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c1bc18b-cfb2-4de9-8c4f-1e361e7a69b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "learning_rate = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 500\n",
    "dataset_size = 30000\n",
    "# max length on input sequence\n",
    "Tx = 30\n",
    "# length of output sequence\n",
    "Ty = 10\n",
    "\n",
    "PATIENCE = 100\n",
    "model_name = f\"noise_{EPOCHS}_{learning_rate}.pth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24f418fb-f3c5-453e-9ac8-0de947aa1dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 30000/30000 [00:01<00:00, 28205.79it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "769f6392-c1d7-4a1b-9131-8a11d56aa8e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wednesday 28 novemBeR1 962', '1962-11-28'),\n",
       " ('Mon JuL 15 1901', '1901-07-15'),\n",
       " ('8 aUG  2024', '2024-08-08'),\n",
       " ('04.02.45', '1945-02-04'),\n",
       " ('558019', '1955-08-19'),\n",
       " ('03-30-29', '1929-03-30'),\n",
       " ('19 85 JUne', '1985-06-19'),\n",
       " ('sUnDay 1 juen 1924', '1924-06-01'),\n",
       " ('mOnday octOber 7 1918', '1918-10-07'),\n",
       " ('16 maY 1965', '1965-05-16')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84100510-6d9c-48c1-a4ec-60d3366dd2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_org, Y_org, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
    "# Xoh and Yoh is one hot representation X and Y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "772bb985-7472-4c70-a4a6-30895fb04ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1795/1021108513.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y = torch.tensor(Y_org, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "Xoh = torch.tensor(Xoh, dtype=torch.float32)\n",
    "Y = torch.tensor(Y_org, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f4ee229-394f-4827-95cd-56b460a708b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dynamic_dataset(Dataset):\n",
    "    def __init__(self, dataset_size):\n",
    "        self.dataset_size = dataset_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.dataset_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X, Y, _ = load_date()\n",
    "        X = torch.tensor(string_to_int(X, Tx, human_vocab))\n",
    "        Y = torch.tensor(string_to_int(Y, Ty, machine_vocab))\n",
    "\n",
    "        Xoh = np.array(list(map(lambda x: one_hot(x, num_classes=len(human_vocab)), X))).astype('float64')\n",
    "        \n",
    "        return torch.tensor(Xoh, dtype=torch.float32).to(device), torch.tensor(Y, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "479319ab-d095-4497-8d56-bae7cbd78edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dynamic_dataset(dataset_size)\n",
    "\n",
    "data_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "121583b3-8e41-4b9b-85a4-41500f0ac636",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_a = 32         # hidden units for pre_attention_lstm/encoder\n",
    "n_s = 64         # hidden units for post_attention_lstm/decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e846884f-51c9-476c-9e29-6fe25c1f3af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat vector for attention mechanism\n",
    "class RepeatVector(nn.Module):\n",
    "    def __init__(self, n):\n",
    "        super(RepeatVector, self).__init__()\n",
    "        self.n = n\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, features)\n",
    "        return x.unsqueeze(1).repeat(1, self.n, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b6a1735-8fdd-48a2-aaa6-5aa9b1a41883",
   "metadata": {},
   "outputs": [],
   "source": [
    "class modelf(nn.Module):\n",
    "    def __init__(self, Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
    "        super(modelf, self).__init__()\n",
    "\n",
    "        self.Tx = Tx\n",
    "        self.Ty = Ty\n",
    "        self.n_a = n_a\n",
    "        self.n_s = n_s\n",
    "        self.human_vocab_size = human_vocab_size\n",
    "        self.machine_vocab_size = machine_vocab_size\n",
    "        \n",
    "        # one_step_attention layers:\n",
    "        self.repeator = RepeatVector(self.Tx)\n",
    "        repeator = RepeatVector(self.Tx)\n",
    "        self.linear1 = nn.Linear((2*self.n_a) + self.n_s, 10)\n",
    "        self.linear2 = nn.Linear(10, 1)\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        # forward layers\n",
    "        self.pre_attention_lstm = nn.LSTM(input_size=self.human_vocab_size, hidden_size=self.n_a, batch_first=True, bidirectional=True)\n",
    "        self.post_attention_lstm = nn.LSTM(2* self.n_a , self.n_s, batch_first=True)\n",
    "        self.output_layer = nn.Linear(self.n_s, self.machine_vocab_size)\n",
    "        self.softmax_main = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def one_step_attention(self, a, s_prev):\n",
    "        # this attention mechnism for lstm\n",
    "        \n",
    "        # add's dimenison 1 making (1,2) to (1, 3, 2)\n",
    "        s_prev = self.repeator(s_prev)\n",
    "        concat = torch.cat([a, s_prev],dim=-1)\n",
    "       \n",
    "        e = self.linear1(concat)\n",
    "        e = self.tanh(e)\n",
    "        \n",
    "        energies = self.linear2(e)\n",
    "        energies = self.relu(energies)\n",
    "        # softmax on dimension 1\n",
    "        alphas = self.softmax(energies)\n",
    "        # this is dot product\n",
    "        context = torch.sum(alphas*a, dim=1, keepdim=True)\n",
    "        \n",
    "        return context\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        a, _ = self.pre_attention_lstm(X)\n",
    "\n",
    "        batch_size = X.shape[0]\n",
    "        s = torch.zeros(batch_size, n_s).to(device)\n",
    "        c = torch.zeros(batch_size, n_s).to(device)\n",
    "\n",
    "        for t in range(self.Ty):\n",
    "            # one setp attention\n",
    "            context = self.one_step_attention(a, s)\n",
    "        \n",
    "            _, (s, c) =  self.post_attention_lstm(context, (s.unsqueeze(0), c.unsqueeze(0)))\n",
    "            s, c = s.squeeze(0), c.squeeze(0)\n",
    "            \n",
    "            out = self.output_layer(s)\n",
    "            \n",
    "            outputs.append(out)\n",
    "\n",
    "        \n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        return outputs\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9165302-b11b-4fee-9419-8d29d2704599",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_vocab_size = len(human_vocab)\n",
    "machine_vocab_size = len(machine_vocab)\n",
    "model = modelf(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "564e33fb-e869-4bf8-a3ee-5dbf0bf695a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "modelf                                   [30000, 10, 11]           --\n",
       "├─LSTM: 1-1                              [30000, 30, 64]           24,064\n",
       "├─RepeatVector: 1-2                      [30000, 30, 64]           --\n",
       "├─Linear: 1-3                            [30000, 30, 10]           1,290\n",
       "├─Tanh: 1-4                              [30000, 30, 10]           --\n",
       "├─Linear: 1-5                            [30000, 30, 1]            11\n",
       "├─ReLU: 1-6                              [30000, 30, 1]            --\n",
       "├─Softmax: 1-7                           [30000, 30, 1]            --\n",
       "├─LSTM: 1-8                              [30000, 1, 64]            33,280\n",
       "├─Linear: 1-9                            [30000, 11]               715\n",
       "├─RepeatVector: 1-10                     [30000, 30, 64]           --\n",
       "├─Linear: 1-11                           [30000, 30, 10]           (recursive)\n",
       "├─Tanh: 1-12                             [30000, 30, 10]           --\n",
       "├─Linear: 1-13                           [30000, 30, 1]            (recursive)\n",
       "├─ReLU: 1-14                             [30000, 30, 1]            --\n",
       "├─Softmax: 1-15                          [30000, 30, 1]            --\n",
       "├─LSTM: 1-16                             [30000, 1, 64]            (recursive)\n",
       "├─Linear: 1-17                           [30000, 11]               (recursive)\n",
       "├─RepeatVector: 1-18                     [30000, 30, 64]           --\n",
       "├─Linear: 1-19                           [30000, 30, 10]           (recursive)\n",
       "├─Tanh: 1-20                             [30000, 30, 10]           --\n",
       "├─Linear: 1-21                           [30000, 30, 1]            (recursive)\n",
       "├─ReLU: 1-22                             [30000, 30, 1]            --\n",
       "├─Softmax: 1-23                          [30000, 30, 1]            --\n",
       "├─LSTM: 1-24                             [30000, 1, 64]            (recursive)\n",
       "├─Linear: 1-25                           [30000, 11]               (recursive)\n",
       "├─RepeatVector: 1-26                     [30000, 30, 64]           --\n",
       "├─Linear: 1-27                           [30000, 30, 10]           (recursive)\n",
       "├─Tanh: 1-28                             [30000, 30, 10]           --\n",
       "├─Linear: 1-29                           [30000, 30, 1]            (recursive)\n",
       "├─ReLU: 1-30                             [30000, 30, 1]            --\n",
       "├─Softmax: 1-31                          [30000, 30, 1]            --\n",
       "├─LSTM: 1-32                             [30000, 1, 64]            (recursive)\n",
       "├─Linear: 1-33                           [30000, 11]               (recursive)\n",
       "├─RepeatVector: 1-34                     [30000, 30, 64]           --\n",
       "├─Linear: 1-35                           [30000, 30, 10]           (recursive)\n",
       "├─Tanh: 1-36                             [30000, 30, 10]           --\n",
       "├─Linear: 1-37                           [30000, 30, 1]            (recursive)\n",
       "├─ReLU: 1-38                             [30000, 30, 1]            --\n",
       "├─Softmax: 1-39                          [30000, 30, 1]            --\n",
       "├─LSTM: 1-40                             [30000, 1, 64]            (recursive)\n",
       "├─Linear: 1-41                           [30000, 11]               (recursive)\n",
       "├─RepeatVector: 1-42                     [30000, 30, 64]           --\n",
       "├─Linear: 1-43                           [30000, 30, 10]           (recursive)\n",
       "├─Tanh: 1-44                             [30000, 30, 10]           --\n",
       "├─Linear: 1-45                           [30000, 30, 1]            (recursive)\n",
       "├─ReLU: 1-46                             [30000, 30, 1]            --\n",
       "├─Softmax: 1-47                          [30000, 30, 1]            --\n",
       "├─LSTM: 1-48                             [30000, 1, 64]            (recursive)\n",
       "├─Linear: 1-49                           [30000, 11]               (recursive)\n",
       "├─RepeatVector: 1-50                     [30000, 30, 64]           --\n",
       "├─Linear: 1-51                           [30000, 30, 10]           (recursive)\n",
       "├─Tanh: 1-52                             [30000, 30, 10]           --\n",
       "├─Linear: 1-53                           [30000, 30, 1]            (recursive)\n",
       "├─ReLU: 1-54                             [30000, 30, 1]            --\n",
       "├─Softmax: 1-55                          [30000, 30, 1]            --\n",
       "├─LSTM: 1-56                             [30000, 1, 64]            (recursive)\n",
       "├─Linear: 1-57                           [30000, 11]               (recursive)\n",
       "├─RepeatVector: 1-58                     [30000, 30, 64]           --\n",
       "├─Linear: 1-59                           [30000, 30, 10]           (recursive)\n",
       "├─Tanh: 1-60                             [30000, 30, 10]           --\n",
       "├─Linear: 1-61                           [30000, 30, 1]            (recursive)\n",
       "├─ReLU: 1-62                             [30000, 30, 1]            --\n",
       "├─Softmax: 1-63                          [30000, 30, 1]            --\n",
       "├─LSTM: 1-64                             [30000, 1, 64]            (recursive)\n",
       "├─Linear: 1-65                           [30000, 11]               (recursive)\n",
       "├─RepeatVector: 1-66                     [30000, 30, 64]           --\n",
       "├─Linear: 1-67                           [30000, 30, 10]           (recursive)\n",
       "├─Tanh: 1-68                             [30000, 30, 10]           --\n",
       "├─Linear: 1-69                           [30000, 30, 1]            (recursive)\n",
       "├─ReLU: 1-70                             [30000, 30, 1]            --\n",
       "├─Softmax: 1-71                          [30000, 30, 1]            --\n",
       "├─LSTM: 1-72                             [30000, 1, 64]            (recursive)\n",
       "├─Linear: 1-73                           [30000, 11]               (recursive)\n",
       "├─RepeatVector: 1-74                     [30000, 30, 64]           --\n",
       "├─Linear: 1-75                           [30000, 30, 10]           (recursive)\n",
       "├─Tanh: 1-76                             [30000, 30, 10]           --\n",
       "├─Linear: 1-77                           [30000, 30, 1]            (recursive)\n",
       "├─ReLU: 1-78                             [30000, 30, 1]            --\n",
       "├─Softmax: 1-79                          [30000, 30, 1]            --\n",
       "├─LSTM: 1-80                             [30000, 1, 64]            (recursive)\n",
       "├─Linear: 1-81                           [30000, 11]               (recursive)\n",
       "==========================================================================================\n",
       "Total params: 59,360\n",
       "Trainable params: 59,360\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 32.25\n",
       "==========================================================================================\n",
       "Input size (MB): 216.00\n",
       "Forward/backward pass size (MB): 1432.80\n",
       "Params size (MB): 0.24\n",
       "Estimated Total Size (MB): 1649.04\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model , input_size = Xoh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b38a3de5-fe5a-450f-bfa0-54bed5aabcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/poorvesh_c/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e864bfb-8a07-4f99-b27d-73b623c3c87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1795/3777675506.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(Xoh, dtype=torch.float32).to(device), torch.tensor(Y, dtype=torch.long).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, loss:0.6919, accuracy: 0.0033%\n",
      "Learning rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(inputs)\n",
    "\n",
    "        output_flat = out.reshape(-1, out.shape[-1])\n",
    "        labels_flat = labels.reshape(-1)\n",
    "\n",
    "        loss = criterion(output_flat, labels_flat)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(out, dim=-1)\n",
    "        correct += (preds == labels).all(dim=1).sum().item()  # full‐sequence matches\n",
    "        total += labels.size(0)\n",
    "        \n",
    "    avg_loss = total_loss / data_loader.__len__()\n",
    "    \n",
    "    if epoch % 50 == 0 or epoch == EPOCHS-1:\n",
    "        print(f\"epoch:{epoch}, loss:{round(total_loss/1000, 4)}, accuracy: {correct/total*100:.4f}%\")\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print(f\"Learning rate: {param_group['lr']}\")\n",
    "\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), f\"dynamic_models/{model_name}\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae10e15-88ca-4dc7-949e-aab939699f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"dynamic_models/final_{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a826a4ea-ab61-47e7-a315-d6c9dfed71a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, sentence):\n",
    "    model.eval()\n",
    "    x_enc = string_to_int(sentence, Tx, human_vocab)\n",
    "    x_enc = np.array(list(map(lambda x: np.eye(len(human_vocab))[x], x_enc)))\n",
    "    X_tensor = torch.tensor(x_enc, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_tensor)\n",
    "        if isinstance(outputs, list):\n",
    "            outputs = torch.stack(outputs, dim=1)\n",
    "        preds = torch.argmax(outputs, dim=-1).squeeze(0).cpu().numpy()\n",
    "    return ''.join(int_to_string(preds, inv_machine_vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aab44b-197c-4c31-8b96-ef89ca0de188",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = translate(model, \"19 3 2003\")\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d1678e-e01a-42a1-940f-f6e72f398efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = ['3 May 1979', '5 Apr 09', '20th Feb 2016', 'Wed 10 Jul 2007', '30 SEPT 2027', '1 jnu 2030']\n",
    "for example in examples:\n",
    "    print(f\"Input: {example} -> Output: {translate(model, example)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea0a916-8420-4f02-832a-248a7dd18a81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
